{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OqlMprHqTzB",
        "outputId": "c1aea07c-e11b-46cb-bfe4-dd852963f9cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 31.4 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=f7af66000276d254e7ff15b19ae2fa056c37f6b3de39e8641041057b05a79935\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=155178 sha256=443b3d5a4dc29b3a5d51bb081e8ed97e1b45cf27098d5e2548843195637e837b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "import pandas as pd\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def acronyms(txt):\n",
        "  normalizer = Normalizer()\n",
        "  txt=normalizer.normalize(txt)\n",
        "  txt = re.sub(r\"[A-Za-z«»,:;</()\\[\\d+>]\", \"\", txt)\n",
        "  #print(txt)\n",
        "  matches = re.findall(\"([\\s]([آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهیئ۱۲۳۴۵۶۷۸۹۰| ]{1,6}\\.){2,4})\", txt)\n",
        "  for macth in matches:\n",
        "  #  print(macth[0])\n",
        "    y= macth[0]\n",
        "    z= y.replace('. ', '\\u200c')\n",
        "    z= z.replace('.', '')\n",
        "  #  print(z)\n",
        "    txt = txt.replace(y, z)\n",
        "  #  print(txt)\n",
        "\n",
        "  return txt"
      ],
      "metadata": {
        "id": "mRt3gH0itsgH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "linesnew = []\n",
        "output = open('voa_fa.txt', \"w\", encoding = \"utf-8\", errors='ignore') \n",
        "with open(\"voa_fa_2003-2008_orig.txt\", \"r\", encoding = \"utf-8\", errors='ignore') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "     \n",
        "        line = f.readline()\n",
        "        line=acronyms(line)\n",
        "        linesnew.append(line)\n",
        "        output.write(line)\n",
        "\n",
        "\n",
        "output.close()"
      ],
      "metadata": {
        "id": "XDoK1J5_tyNe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  print(linesnew[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSwgDXUrVJEU",
        "outputId": "ad8952b8-88ad-4f27-a142-49ab5b46bfee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "در یکی از این گزارشها، که از سوی خبرگزاری کیودو، انتشار یافته، گفته شده است که دو کشور برای رفع اختلافات دیرین خود بر سر چهار جزیره از جزایر زنجیره‌ای کوریل، بر اساس سه پیمان گذشته خود عمل خواهند کرد. بموجب یکی از این پیمانها که در سال  امضاء شده، دو تا از این جزیره‌ها پس از امضاء یک پیمان صلح به ژاپن پس داده خواهد شد. اما بموجب پیمانی که در سال  به امضاء رسیده، مسئله حاکمیت این چهار جزیره بایستی پیش از امضاء پیمان صلح فیصله یابد. هیچ یک از دو طرف نحوه استفاده از پیمان‌های پیشین را اعلام نکرده‌اند. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = []\n",
        "for sentence in linesnew:\n",
        "    sentence= word_tokenize(sentence) \n",
        "    for token in sentence:\n",
        "       \n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n"
      ],
      "metadata": {
        "id": "8Mg0BPURq5FE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary[35000]"
      ],
      "metadata": {
        "id": "aHGdF0cOUjI8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6729fd63-118e-4fc8-e911-5c9577aa2f88"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'بلاگ\\u200cهای'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "idx_pairs = []\n",
        "# for each sentence\n",
        "for sentence in linesnew:\n",
        "    sentence= word_tokenize(sentence) \n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "    # for each word, threated as center word\n",
        "    for center_word_pos in range(len(indices)):\n",
        "        # for each window position\n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            # make soure not jump out sentence\n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
      ],
      "metadata": {
        "id": "bKsIKHxGWAQv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6s6HIOeWlyO",
        "outputId": "cd33ecea-893d-4c61-e871-bdaa39942b24"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   1],\n",
              "       [  1,   0],\n",
              "       [  0,   2],\n",
              "       ...,\n",
              "       [562,  32],\n",
              "       [ 32, 681],\n",
              "       [ 32, 562]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_layer(word_idx):\n",
        "    x = torch.zeros(vocabulary_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x"
      ],
      "metadata": {
        "id": "01acL397W5D1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_dims = 5\n",
        "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
        "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epo in range(num_epochs):\n",
        "    loss_val = 0\n",
        "    for data, target in idx_pairs:\n",
        "        x = Variable(get_input_layer(data)).float()\n",
        "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "\n",
        "        z1 = torch.matmul(W1, x)\n",
        "        z2 = torch.matmul(W2, z1)\n",
        "    \n",
        "        log_softmax = F.log_softmax(z2, dim=0)\n",
        "\n",
        "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        loss_val += loss.item()\n",
        "        #total_loss += loss_val.data\n",
        "        \n",
        "        loss.backward()\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "    if epo % 10 == 0:    \n",
        "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
      ],
      "metadata": {
        "id": "sw5x7YztW9ST"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}